import { ChatEngine } from './utils/ChatEngine';
import { SendMessageBody } from './requests/sendMessage.request';
import { forwardRef, Inject, Injectable } from '@nestjs/common';
import { RedisClient } from 'providers/RedisClient';
import { ChatOpenAI } from '@langchain/openai';
import { BaseChatModel } from '@langchain/core/dist/language_models/chat_models';
import { vars } from '../../config/vars';
import { IResponseWithActions, LangChainChatEngine } from './utils/LangChainChatEngine';
import { FineTunedModels } from './utils/LLMEngine';
import { TelegramAPI } from 'providers/Telegram';
import { MyLogger } from 'config/MyLogger';
import { QdrantProvider } from 'providers/QdrantClient';
import { PERSON_DIALOGS_COLLECTION } from '../../common/const/VECTOR_COLLECTIONS_NAMES';
import { YandexMLProvider } from 'providers/YandexML';
import { InjectModel } from '@nestjs/mongoose';
import { DialogStats, DialogStatsDocument } from 'schemas/dialogStats.scheme';
import mongoose, { Model } from 'mongoose';
import { llmContextToVectorData } from '../dialogsData/utils/llmContextToVectorData';
import { ChatGenerationPromptInputs, MessengerPrompts } from './utils/MessengerPrompt';
import { getObjFromLLM } from './utils/getObjFromLLM';
// eslint-disable-next-line @typescript-eslint/no-require-imports
import TelegramBot = require('node-telegram-bot-api');


@Injectable()
export class ChatService {
    constructor(
        @Inject(RedisClient)
        private readonly redisClient: RedisClient,
        @Inject(forwardRef(() => TelegramAPI))
        private readonly telegram: TelegramAPI,
        @Inject(forwardRef(() => MyLogger))
        private readonly logger: MyLogger,
        @Inject(forwardRef(() => QdrantProvider))
        private readonly qdrantProvider: QdrantProvider,
        @Inject(forwardRef(() => YandexMLProvider))
        private readonly yandexML: YandexMLProvider,
        @InjectModel(DialogStats.name)
        private dialogStatsModel: Model<DialogStatsDocument>,
    ){}
    
    llm:BaseChatModel;
    chatEngine:ChatEngine;
    
    async sendMessage(body:SendMessageBody):Promise<any> {
        const result = await this.onReceiveMessage({
            text: body.text,
            chatId: body.sessionId,
            userId: vars.meUserId,
        });

        return {
            response: result,
        };
    }

    private async getSimilarMessages(dialogPart:string, personId:string):Promise<string[]> {
        const queryVector = await this.yandexML.embeddings.embedQuery(`–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: ${dialogPart}`);

        const result = await this.qdrantProvider.client.query(PERSON_DIALOGS_COLLECTION({
            personId,
        }), {
            query: queryVector,
            params: {
                hnsw_ef: 128,
                exact: false,
            },
            with_payload: true,
            limit: 2,
        });

        return result.points.map((p) => {
            return (p.payload as { text: string }).text;
        });
    }

    async onBotMessageReceived(message:TelegramBot.Message):Promise<void> {
        if (!message.from?.id || !message.text) {
            return;
        }

        this.logger.log('Received text message from bot:', message);

        const responseText = await this.onReceiveMessage({
            text: message.text,
            chatId: message.chat.id.toString(),
            userId: vars.meUserId,
        });

        await this.telegram.client.sendMessage(message.chat.id, responseText);
    }

    private async getPrompt({ 
        rag1,
        rag2,
        isCorrectionNeeded,
        previousGeneratedResponse,
        verifierLlmFeedback,
        currentChatHistory,
    }:{
        rag1: string,
        rag2: string,
        isCorrectionNeeded: boolean,
        previousGeneratedResponse?: string,
        verifierLlmFeedback?: string,
        currentChatHistory: string,
    }) {
        const messengerPrompts = new MessengerPrompts();

        const initialInputs: ChatGenerationPromptInputs = {
            userNameOrNickname: '–ù–∏–∫–∏—Ç–∞',
            userPersonalityDetails: '–ú–Ω–µ 23 –≥–æ–¥–∞ —è –º—É–∂—á–∏–Ω–∞. –ñ–∏–≤—É –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–µ, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—É. –õ—é–±–ª—é —à—É—Ç–∏—Ç—å (—á–∞—Å—Ç–æ –∏—Ä–æ–Ω–∏–∑–∏—Ä—É—é), —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é —ç–º–æ–¥–∑–∏. –û—Ç–≤–µ—á–∞—é –∫—Ä–∞—Ç–∫–æ, –Ω–æ –ø–æ –¥–µ–ª—É. –ò–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º, AI, –≤–µ–ª–æ—Å–∏–ø–µ–¥–∞–º–∏.',
            ragExample1: rag1,
            ragExample2: rag2,
            isCorrectionNeeded: false,
            currentChatHistory,
        };

        if (isCorrectionNeeded) {
            initialInputs.isCorrectionNeeded = true;
            initialInputs.previousGeneratedResponse = previousGeneratedResponse;
            initialInputs.verifierLlmFeedback = verifierLlmFeedback;
        }

        return messengerPrompts.formatChatPrompt(initialInputs);
    };

    private async evaluateLLMAnswer({ currentDialog }:{ currentDialog: string }):Promise<{
        shouldRetry: boolean,
        retryDesc: string,
    }> {
        const model = new ChatOpenAI({
            model: 'deepseek-ai/DeepSeek-V3',
            configuration: {
                baseURL: vars.nebius.baseUrl,
                apiKey: vars.nebius.secretKey,
            },
            temperature: 0.1,
        });

        const prompt = '–£ —Ç–µ–±—è –µ—Å—Ç—å —á–∞—Å—Ç—å –ø–µ—Ä–µ–ø–µ—Å–∫–∏, –≥–¥–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ LLM,' +
            ' —Ç–µ–±–µ –Ω—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è. –°–æ–æ–±—â–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–ª–æ–≥–∏—á–Ω—ã–º, ' +
            '–µ—Å–ª–∏ –Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–Ω—è—Ç—å –µ–≥–æ —Å–≤—è–∑—å —Å –æ—Å—Ç–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–ø–∏—Å–∫–æ–π. –û—Ç–≤–µ—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–ª–∏ –∏—Ä–æ–Ω–∏—á–Ω—ã–º–∏.' +
            ' –û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –æ–±—ä–µ–∫—Ç–∞:' +
            ' { "shouldRetry": boolean, "retryDesc": string }. shouldRetry - –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç, retryDesc -' +
            ' –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è LLM –ø–æ—á–µ–º—É –æ—Ç–≤–µ—Ç –Ω–µ –≤–µ—Ä–Ω—ã–π. –í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç –≤–∏–¥–∞ { "shouldRetry": boolean, "retryDesc": string }' +

            '–ü—Ä–∏–º–µ—Ä 1: \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –ù–µ—Ç'+
            '–Ø: —Ç—ã –≥–¥–µ  \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –ò–¥—É  \n' +
            '–Ø: –∫—É–¥–∞....  \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –î–æ–º–æ–π' +
            '–Ø: –Ω–∏—á–µ–≤–æ –Ω–µ –∑–∞–±—ã–ª ? üò•' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –° –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è –í—Å–ø–æ–º–Ω–∏–ª... –ù–æ —É –º–µ–Ω—è –±–∞—à–∫–∞ —â–∞ –ª–æ–ø–Ω–µ—Ç... –û—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\\n\' +\n' +
            '–Ø: –õ–∞–¥–Ω–æ –°–∏–¥–∏ –æ—Å–º—ã—Å–ª—è–π –£—Å–ª—ã—à–∞–Ω–Ω–æ–µ  \n' +
            '–û—Ç–≤–µ—Ç 1 { "shouldRetry": false, "retryDesc": "" }  \n' +

            '–ü—Ä–∏–º–µ—Ä 2: \n' +
            '–Ø: –ß—Ç–æ –≤ —Å—É–±–±–æ—Ç—É –¥–µ–ª–∞–µ—à—å? \n ' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –í —à–∫–æ–ª—É  \n' +
            '–Ø: –¥–æ —Å–∫–æ–ª—å–∫–∏?  \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: –î–æ 2.30  \n' +
            '–Ø: –¥–∞–≤–∞–π –≤ —Å–µ—Å—Ç—Ä–æ—Ä–µ—Ü–∫ –ø–æ—Å–ª–µ  \n' +
            '–Ø: –ú–æ–∂–Ω–æ  \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:–≤—Å–µ, —Ç–æ–≥–¥–∞ –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º—Å—è?  \n' +
            '–Ø: –í —à–∫–æ–ª–µ  \n' +
            '–°–æ–±–µ—Å–µ–¥–Ω–∏–∫:...  \n' +
            '–Ø: —Ç–∞–º –ø–æ–π–¥—ë–º –≤ –∫–∏–Ω–æ  \n' +
            '–û—Ç–≤–µ—Ç 2: { "shouldRetry": true, "retryDesc": "–ü–æ–π–¥–µ–º –≤ –∫–∏–Ω–æ –Ω–µ —Å–≤—è–∑–∞–Ω–æ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–∏–∞–ª–æ–≥–∞ (–æ–Ω–æ –ª–æ–º–∞–µ—Ç –µ–≥–æ —Å–º—ã—Å–ª–∞), —à—É—Ç–∫–æ–π –∏–ª–∏ –∏—Ä–æ–Ω–∏–µ–π —ç—Ç–æ –Ω–µ–ª—å–∑—è –Ω–∞–∑–≤–∞—Ç—å" } \n' +
            '–¢–µ–ø–µ—Ä—å –æ—Ü–µ–Ω–∏ —ç—Ç–æ—Ç –¥–∏–∞–ª–æ–≥: \n' + currentDialog;

        const aiMsg = await model.invoke(prompt);
        const result = getObjFromLLM({
            llmResult: aiMsg.content as string,
        });

        if (result.isValid) {
            return {
                shouldRetry: result.obj.shouldRetry || false,
                retryDesc: result.obj.retryDesc || '',
            };
        }
        return {
            shouldRetry: false,
            retryDesc: '',
        };
    }

    private async tryGetAnswerFromLLM({ chatId, prompt, humanMessage }:{ chatId: string, prompt: string, humanMessage: string }):Promise<IResponseWithActions> {
        this.llm = new ChatOpenAI({
            configuration: {
                baseURL: vars.nebius.baseUrl,
                apiKey: vars.nebius.secretKey,
            },
            model: FineTunedModels.BashirLlama70b,
            temperature: 0.8,
            topP: 0.6,
        });

        const chain = new LangChainChatEngine({
            llm: this.llm,
            sessionId: chatId,
        });

        return await chain.invoke({ prompt, humanMessage });
    }

    private async getAnswerFromLLm({
        retryLimit,
        currentRetry,
        chatId,
        rag2,
        rag1,
        isCorrectionNeeded,
        humanMessage,
        previousGeneratedResponse,
        verifierLlmFeedback,
        lastDialogHistoryInText,
        currentChatHistory,
    }:{
        retryLimit: number,
        currentRetry: number,
        chatId: string,
        rag1: string,
        rag2: string,
        lastDialogHistoryInText: string,
        humanMessage: string,
        currentChatHistory: string,
        isCorrectionNeeded?: boolean,
        previousGeneratedResponse?: string,
        verifierLlmFeedback?: string,
    }):Promise<string> {
        try {
            const prompt = await this.getPrompt({
                rag1,
                rag2,
                isCorrectionNeeded: isCorrectionNeeded || false,
                previousGeneratedResponse,
                verifierLlmFeedback,
                currentChatHistory,
            });

            console.log(prompt);

            const llmAnswer = await this.tryGetAnswerFromLLM({
                chatId,
                prompt,
                humanMessage,
            });

            const resultAnswer = await this.evaluateLLMAnswer({
                currentDialog: lastDialogHistoryInText + ` \n –Ø: ${llmAnswer.response}`,
            });

            if (resultAnswer.shouldRetry && (retryLimit > currentRetry)) {
                this.logger.log(`retry ${currentRetry}, answer: ${llmAnswer.response}, feedback ${resultAnswer.retryDesc}`);
                return this.getAnswerFromLLm({
                    retryLimit,
                    currentRetry: currentRetry + 1,
                    chatId,
                    rag2,
                    rag1,
                    isCorrectionNeeded: true,
                    previousGeneratedResponse: llmAnswer.response,
                    verifierLlmFeedback: resultAnswer.retryDesc,
                    lastDialogHistoryInText,
                    humanMessage,
                    currentChatHistory,
                });
            }

            console.log('try save');
            await llmAnswer.onSaveContext();

            return llmAnswer.response;
        } catch (e) {
            this.logger.error('error get Answer from llm', e);
            if (retryLimit > currentRetry) {
                return this.getAnswerFromLLm({
                    retryLimit,
                    currentRetry: currentRetry + 1,
                    chatId,
                    rag2,
                    rag1,
                    isCorrectionNeeded,
                    previousGeneratedResponse,
                    verifierLlmFeedback,
                    lastDialogHistoryInText,
                    humanMessage,
                    currentChatHistory,
                });
            }

            return '';
        }
    };

    private async onReceiveMessage({ text, chatId, userId }:{ text: string, chatId: string, userId: string }):Promise<string> {
        const selectedDialogStats = await this.dialogStatsModel.findOne({
            ownerUserId: new mongoose.Types.ObjectId(userId),
            isSelected: true,
        });

        if (!selectedDialogStats) {
            return '';
        }

        const dialogHistory = await this.redisClient.client.lrange(chatId, 0, -1);
        const parsedDialogHistory = dialogHistory.map((d) => JSON.parse(d));
        let lastDialogHistoryInText = llmContextToVectorData(parsedDialogHistory.slice(0, 30).reverse());
        lastDialogHistoryInText = `${lastDialogHistoryInText}\n –°–æ–±–µ—Å–µ–¥–Ω–∏–∫:${text} \n`;

        const similarMessages = await this.getSimilarMessages(lastDialogHistoryInText, selectedDialogStats.personId);

        return this.getAnswerFromLLm({
            retryLimit: 10,
            currentRetry: 1,
            chatId,
            rag2: similarMessages[1] || '',
            rag1: similarMessages[0] || '',
            isCorrectionNeeded: false,
            lastDialogHistoryInText,
            humanMessage: text,
            currentChatHistory: lastDialogHistoryInText,
        });
    }
}